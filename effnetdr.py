# -*- coding: utf-8 -*-
"""EffNetdr.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iRngt5yokyrNFjcEZOGvtDqXcDW5S9Fb
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetB3
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

#from google.colab import drive
#drive.mount('/content/drive')

img_path = "C:\\Users\\Vitor\\Desktop\\resized_train\\resized_train"
#file_path = "/home/vitoroliveira/CNN_Retinopatia_Diabética/"
#img_path = "/home/vitoroliveira/CNN_Retinopatia_Diabética/resized_train/"
csv_name = "trainLabels3.csv"

# Carregar o CSV com os detalhes das imagens
df = pd.read_csv(csv_name)
df = df.rename(columns={'image': 'image_id', 'level': 'dr_grade'})  # Ajuste conforme as colunas do seu CSV

# Converter a coluna 'dr_grade' para string, se ainda não estiver
df['dr_grade'] = df['dr_grade'].astype(str)

# Amostrar uma fração dos dados
df_sampled = df.sample(frac=0.1, random_state=43)

# Dividir o conjunto amostrado em treino, validação e teste
train_df, test_df = train_test_split(df_sampled, test_size=0.15, random_state=43)
train_df, val_df = train_test_split(train_df, test_size=0.15/(1-0.15), random_state=43)

# Verifique as primeiras linhas e as dimensões de cada conjunto
print("Train DataFrame Head:")
print(train_df.head())
print("Train DataFrame Shape:", train_df.shape)

print("\nValidation DataFrame Head:")
print(val_df.head())
print("Validation DataFrame Shape:", val_df.shape)

print("\nTest DataFrame Head:")
print(test_df.head())
print("Test DataFrame Shape:", test_df.shape)

batch_size = 16

# Configurar o ImageDataGenerator para treino e validação com normalização
datagen_train_val = ImageDataGenerator(rescale=1./255)
datagen_test = ImageDataGenerator(rescale=1./255)

# Gerador de treino
train_gen = datagen_train_val.flow_from_dataframe(
    dataframe=train_df,
    directory=img_path,
    x_col="image_id",
    y_col="dr_grade",
    target_size=(224, 224),
    batch_size=batch_size,
    class_mode="categorical"
)

# Gerador de validação
val_gen = datagen_train_val.flow_from_dataframe(
    dataframe=val_df,
    directory=img_path,
    x_col="image_id",
    y_col="dr_grade",
    target_size=(224, 224),
    batch_size=batch_size,
    class_mode="categorical"
)

# Gerador de teste
test_gen = datagen_test.flow_from_dataframe(
    dataframe=test_df,
    directory=img_path,
    x_col="image_id",
    y_col="dr_grade",
    target_size=(224, 224),
    batch_size=batch_size,
    class_mode="categorical",
    shuffle=False  # Para manter a ordem nas previsões
)

# Verificar as dimensões de uma amostra do batch
for images, labels in train_gen:
    print(f"Dimensões das imagens: {images.shape}")
    print(f"Dimensões dos rótulos: {labels.shape}")
    break

# Obter o número de classes
num_classes = len(train_gen.class_indices)  # Número de classes no dataset

# Carregar EfficientNetB3 com pesos pré-treinados no ImageNet
base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Adicionar camadas de pooling e fully connected para classificação
x = base_model.output
x = GlobalAveragePooling2D()(x)  # Redução de dimensionalidade
x = Dense(128, activation='relu')(x)
output = Dense(num_classes, activation='softmax')(x)  # Ajuste para o número de classes no dataset

# Construir o modelo completo
model = Model(inputs=base_model.input, outputs=output)

# Congelar as camadas do modelo base para preservar o aprendizado prévio
for layer in base_model.layers:
    layer.trainable = False

from tensorflow.keras.mixed_precision import set_global_policy

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

import os
# Caminho para o checkpoint salvo (ajuste conforme necessário)
checkpoint_path = 'model_checkpoint.weights.h5'
#checkpoint_path = '/home/vitoroliveira/CNN_Retinopatia_Diabética/model_checkpoint.keras'

# Verificar se o checkpoint existe e carregar os pesos se existir
if os.path.exists(checkpoint_path):
    try:
        model.load_weights(checkpoint_path)
        print("Checkpoint carregado com sucesso.")
    except Exception as e:
        print(f"Erro ao carregar checkpoint: {e}")
else:
    print("Checkpoint não encontrado. Iniciando o treinamento do zero.")

# Definir o callback para salvar o modelo atualizado
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path,
    save_best_only=True,
    monitor='val_loss',
    mode='min',
    save_weights_only=True
)

# Definir o callback para early stopping
early_stopping_callback = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

import math

steps_per_epoch = math.ceil(train_gen.n / train_gen.batch_size)
validation_steps = math.ceil(val_gen.n / val_gen.batch_size)

epochs = 10  # Ajuste o número de épocas conforme necessário

history = model.fit(
    train_gen,
    #steps_per_epoch=steps_per_epoch,
    validation_data=val_gen,
    #validation_steps=validation_steps,
    epochs=epochs,
    callbacks=[checkpoint_callback, early_stopping_callback]
)

# Avaliação no conjunto de teste
test_loss, test_accuracy = model.evaluate(test_gen)
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")

# Obter as previsões e as classes reais para o cálculo do coeficiente kappa
y_true = test_gen.classes
y_pred = np.argmax(model.predict(test_gen), axis=-1)

# Calcular o coeficiente kappa
kappa = cohen_kappa_score(y_true, y_pred, weights='quadratic')
print(f"Cohen's Kappa: {kappa}")

import matplotlib.pyplot as plt

# Plotar Loss e Acurácia
plt.figure(figsize=(12, 5))

# Loss
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Acurácia
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

# Descongelar as últimas camadas do modelo base para fine-tuning
for layer in base_model.layers[-20:]:
    layer.trainable = True

# Compilar novamente com uma taxa de aprendizado menor
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Treinamento de ajuste fino
fine_tune_epochs = 5
history_fine = model.fit(
    train_gen,
    steps_per_epoch=train_gen.samples // train_gen.batch_size,
    validation_data=val_gen,
    validation_steps=val_gen.samples // val_gen.batch_size,
    epochs=fine_tune_epochs
)